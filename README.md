# ğŸ§  AI Exam Corrector

An intelligent, end-to-end exam grading system that combines **OCR**, **BERT semantic similarity**, and **LLM reasoning** to automatically grade handwritten or typed student exam papers â€” with constructive feedback.

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python)
![Streamlit](https://img.shields.io/badge/UI-Streamlit-FF4B4B?logo=streamlit)
![License](https://img.shields.io/badge/License-MIT-green)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Configuration](#configuration)
- [Usage](#usage)
- [Datasets](#datasets)
- [How Grading Works](#how-grading-works)
- [Contributing](#contributing)
- [License](#license)

---

## Overview

Traditional exam grading is time-consuming and subjective. **AI Exam Corrector** automates this process using a hybrid approach:

1. **OCR** extracts student answers from scanned exam sheets (images or PDFs).
2. **BERT embeddings** measure semantic similarity between student and reference answers.
3. An **LLM** (Groq / Gemini / OpenRouter) applies expert-level reasoning to assign a score and generate constructive feedback.
4. A **hybrid formula** combines both signals for a balanced, fair final grade.

Teachers only need to configure reference answers once â€” after that, grading is fully automatic.

---

## Features

- **OCR Support** â€” EasyOCR (default) and Tesseract backends for extracting text from images and PDFs.
- **BERT Semantic Scoring** â€” Uses sentence-transformers (`all-MiniLM-L6-v2` by default) for cosine similarity matching.
- **LLM Grading & Feedback** â€” Leverages free LLM APIs (Groq, Google Gemini, OpenRouter) for nuanced scoring and written feedback.
- **Hybrid Scoring Formula** â€” `final_score = Î± Ã— BERT_score + (1 âˆ’ Î±) Ã— LLM_score` with tunable Î±.
- **Streamlit Web UI** â€” Beautiful, dark-themed interface with two modes:
  - **Grade Mode** â€” Upload an exam image and get instant AI-powered scores.
  - **Setup Mode** â€” Configure exam templates with reference answers.
- **Exam Template Management** â€” Save/load exam configurations as JSON for reuse.
- **Dataset Evaluation** â€” Built-in loaders for standard ASAG benchmarks (Mohler, SemEval-2013, ASAP-SAS) with correlation metrics.
- **Detailed Analytics** â€” Per-question breakdowns, score distributions, grade letters, and exportable reports.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Exam Image/PDF  â”‚â”€â”€â”€â”€â–¶â”‚  OCR Engine  â”‚â”€â”€â”€â”€â–¶â”‚  Extracted Answers  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ (EasyOCR /   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚  Tesseract)  â”‚              â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚   Hybrid Grader     â”‚
â”‚ Exam Template   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                     â”‚
â”‚ (ref answers)   â”‚                         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚  â”‚ BERT Scorer   â”‚  â”‚
                                            â”‚  â”‚ (cosine sim)  â”‚  â”‚
                                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                            â”‚          â”‚          â”‚
                                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                            â”‚  â”‚ LLM Feedback  â”‚  â”‚
                                            â”‚  â”‚ (score + text) â”‚  â”‚
                                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                            â”‚          â”‚          â”‚
                                            â”‚  Î±Â·BERT + (1-Î±)Â·LLM â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚   Final Grade +     â”‚
                                            â”‚   Feedback Report   â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
AI_Correcting/
â”œâ”€â”€ app.py               # Streamlit web UI (main entry point)
â”œâ”€â”€ config.py            # Central configuration (API keys, model settings)
â”œâ”€â”€ ocr_engine.py        # OCR backends (EasyOCR, Tesseract)
â”œâ”€â”€ bert_scorer.py       # BERT semantic similarity scoring
â”œâ”€â”€ llm_feedback.py      # LLM-based grading via Groq / Gemini / OpenRouter
â”œâ”€â”€ hybrid_grader.py     # Hybrid grading pipeline (BERT + LLM)
â”œâ”€â”€ exam_manager.py      # Exam template CRUD (JSON-based)
â”œâ”€â”€ dataset_loader.py    # Standard ASAG dataset loaders & evaluation
â”œâ”€â”€ test_pipeline.py     # End-to-end pipeline test script
â”œâ”€â”€ report.tex           # LaTeX project report
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ exams/               # Saved exam templates (JSON)
â”‚   â””â”€â”€ demo_biology.json
â”œâ”€â”€ datasets/            # Downloaded evaluation datasets
â”‚   â””â”€â”€ README.md
â””â”€â”€ samples/             # Sample exam images for testing
```

---

## Getting Started

### Prerequisites

- **Python 3.10+**
- A free API key from at least one LLM provider:
  - [Groq](https://console.groq.com/) (recommended â€” fast & free)
  - [Google Gemini](https://aistudio.google.com/app/apikey)
  - [OpenRouter](https://openrouter.ai/)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/YOUR_USERNAME/AI_Correcting.git
   cd AI_Correcting
   ```

2. **Create a virtual environment (recommended):**
   ```bash
   python -m venv venv
   source venv/bin/activate        # Linux/macOS
   venv\Scripts\activate           # Windows
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables:**

   Create a `.env` file in the project root:
   ```env
   # LLM Provider: "groq", "gemini", or "openrouter"
   LLM_PROVIDER=groq

   # API Keys (add at least one)
   GROQ_API_KEY=your_groq_api_key_here
   GEMINI_API_KEY=your_gemini_api_key_here
   OPENROUTER_API_KEY=your_openrouter_api_key_here

   # Optional tuning
   ALPHA=0.4
   BERT_MODEL_NAME=all-MiniLM-L6-v2
   OCR_ENGINE=easyocr
   OCR_LANGUAGES=en,fr
   ```

---

## Configuration

All settings are managed in `config.py` and can be overridden via environment variables:

| Variable | Default | Description |
|---|---|---|
| `LLM_PROVIDER` | `groq` | LLM backend (`groq`, `gemini`, `openrouter`) |
| `GROQ_API_KEY` | â€” | Groq API key |
| `GEMINI_API_KEY` | â€” | Google Gemini API key |
| `OPENROUTER_API_KEY` | â€” | OpenRouter API key |
| `ALPHA` | `0.4` | Hybrid weight: 0 = pure LLM, 1 = pure BERT |
| `BERT_MODEL_NAME` | `all-MiniLM-L6-v2` | Sentence-transformers model |
| `OCR_ENGINE` | `easyocr` | OCR backend (`easyocr`, `tesseract`) |
| `OCR_LANGUAGES` | `en,fr` | OCR language codes |
| `MAX_SCORE` | `20` | Default maximum score |
| `LLM_TEMPERATURE` | `0.2` | LLM generation temperature |

---

## Usage

### Launch the Web UI

```bash
streamlit run app.py
```

This opens a Streamlit dashboard with two modes:

- **Grade Mode** â€” Upload a scanned exam image or PDF to get instant AI-generated grades and feedback.
- **Setup Mode** â€” Create and manage exam templates by defining questions and reference answers.

### Run the Test Pipeline

```bash
python test_pipeline.py
```

Runs built-in sample Q&A pairs through the full BERT + LLM hybrid grading pipeline and prints detailed results.

---

## Datasets

The system supports evaluation on standard Automated Short Answer Grading (ASAG) benchmarks:

| Dataset | Size | Score Range | Source |
|---|---|---|---|
| **Mohler et al. (2011)** | 2,273 answers | 0â€“5 | [GitHub](https://github.com/lm-pub-quiz/Mohler-dataset) |
| **SemEval-2013 Task 7** | Beetle + SciEntsBank | 5-way labels | [York CS](https://www.cs.york.ac.uk/semeval-2013/task7/) |
| **ASAP-SAS** | ~17k answers | 0â€“2 / 0â€“3 | [Kaggle](https://www.kaggle.com/c/asap-sas/data) |

See [datasets/README.md](datasets/README.md) for download and setup instructions.

---

## How Grading Works

1. **OCR Extraction** â€” The uploaded exam image/PDF is processed by EasyOCR (or Tesseract) to extract student answers as text.

2. **BERT Semantic Similarity** â€” Each student answer is encoded alongside the reference answer using a sentence-transformers model. Cosine similarity is computed and normalized to the score scale.

3. **LLM Expert Grading** â€” The reference answer, student answer, and BERT similarity are sent to an LLM which returns a numeric score and constructive feedback. The LLM considers:
   - Key concept coverage
   - Factual accuracy
   - Partial credit for incomplete answers

4. **Hybrid Score** â€” The final score combines both signals:
   ```
   final_score = Î± Ã— BERT_normalized_score + (1 âˆ’ Î±) Ã— LLM_score
   ```
   Where Î± (default 0.4) balances semantic matching with expert reasoning.

---

## Contributing

Contributions are welcome! To contribute:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/your-feature`)
3. Commit your changes (`git commit -m "Add your feature"`)
4. Push to the branch (`git push origin feature/your-feature`)
5. Open a Pull Request

---

## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.
